From 97932e2358a666b6272055a0f610b6ed1dc3104a Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Mon, 16 Oct 2023 10:30:13 -0400
Subject: [PATCH] Fix: ring buffer: communicate events lost from reserve to commit

In scheduler overcommit scenarios, threads can be preempted between slot
reservation and commit. Therefore, the thread performing the slot
reservation at the end of the subbuffer is not necessarily the same
thread which performs the commit which completely fills the
subbuffer.

Therefore, relying on the lttng_ust_ring_buffer_ctx to communicate the
events lost count between reserve and commit is not appropriate in this
scenarios, because it only communicates information between reserve and
associated commit pairs.

Fix this by communicating records lost values between space reservation
at end of sub-buffer and last commit of sub-buffer through fields within
the per-subbuffer commit_counters_cold structure. Reserve an area of
unused zeroed padding for this purpose.

The records_lost fields are stored after a successful reservation of
the last subbuffer slot, before its associated commit is done: this
guarantees mutual exclusion on the records_lost fields. Then the
records_lost fields are loaded in the buffer_end() callback on subbuffer
delivery, which is invoked only when the subbuffer is fully committed.

This change does not break ring buffer protocol compatibility:

If an application with the "old" behavior (using
lttng_ust_ring_buffer_ctx to communicate information between
last-slot-reserve and associated commit) [A] interacts with an
application using the "new" behavior (using the commit_cold records_lost
fields to communicate information between last-slot-reserve and the
commit doing the last increment in the subbuffer) [B], we end up with
those cases:

Works:

* [A] performs last-slot-reserve and its paired last-slot-commit: Works
  OK, as previously, through lttng_ust_ring_buffer_ctx,
* [B] performs last-slot-reserve and its paired last-slot-commit: Works
  OK, through commit_cold records_lost fields,
* [B] performs last-slot-reserve, preempted, and [B] does non-paired
  last-slot-commit: Works OK, through commit_cold records_lost fields.

Does not work, as before:

* [A] performs last-slot-reserve, preempted, and [A] does non-paired
  last-slot-commit: Does not work, as before.
* [B] performs last-slot-reserve, preempted, and [A] does non-paired
  last-slot-commit: Does not work, as before, because [A] relies
  on lttng_ust_ring_buffer_ctx.

Does not work, new behavior affecting a scenario that did not work
before anyway:

* [A] performs last-slot-reserve, preempted, and [B] does non-paired
  last-slot-commit: Does not work, because [A] does not populate
  commit_cold records_lost fields. [B] may load an outdated
  records_lost fields, which is a new incorrect behavior in this
  mixed versions scenario.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Change-Id: I863e1d47ad5bb84074998be49cf979fca5d27760
---

diff --git a/src/common/ringbuffer-clients/metadata-template.h b/src/common/ringbuffer-clients/metadata-template.h
index 56d9551..b7abefd 100644
--- a/src/common/ringbuffer-clients/metadata-template.h
+++ b/src/common/ringbuffer-clients/metadata-template.h
@@ -147,9 +147,9 @@
 	 * We do not care about the records lost count, because the metadata
 	 * channel waits and retry.
 	 */
-	(void) lib_ring_buffer_get_records_lost_full(&client_config, ctx);
-	records_lost += lib_ring_buffer_get_records_lost_wrap(&client_config, ctx);
-	records_lost += lib_ring_buffer_get_records_lost_big(&client_config, ctx);
+	(void) lib_ring_buffer_get_records_lost_full(&client_config, buf, ctx, subbuf_idx, handle);
+	records_lost += lib_ring_buffer_get_records_lost_wrap(&client_config, buf, ctx, subbuf_idx, handle);
+	records_lost += lib_ring_buffer_get_records_lost_big(&client_config, buf, ctx, subbuf_idx, handle);
 	WARN_ON_ONCE(records_lost != 0);
 }
 
diff --git a/src/common/ringbuffer-clients/template.h b/src/common/ringbuffer-clients/template.h
index fe8f8e0..990b5bf 100644
--- a/src/common/ringbuffer-clients/template.h
+++ b/src/common/ringbuffer-clients/template.h
@@ -418,10 +418,9 @@
 		(uint64_t) data_size * CHAR_BIT;		/* in bits */
 	header->ctx.packet_size =
 		(uint64_t) LTTNG_UST_PAGE_ALIGN(data_size) * CHAR_BIT;	/* in bits */
-
-	records_lost += lib_ring_buffer_get_records_lost_full(&client_config, ctx);
-	records_lost += lib_ring_buffer_get_records_lost_wrap(&client_config, ctx);
-	records_lost += lib_ring_buffer_get_records_lost_big(&client_config, ctx);
+	records_lost += lib_ring_buffer_get_records_lost_full(&client_config, buf, ctx, subbuf_idx, handle);
+	records_lost += lib_ring_buffer_get_records_lost_wrap(&client_config, buf, ctx, subbuf_idx, handle);
+	records_lost += lib_ring_buffer_get_records_lost_big(&client_config, buf, ctx, subbuf_idx, handle);
 	header->ctx.events_discarded = records_lost;
 }
 
diff --git a/src/common/ringbuffer/frontend.h b/src/common/ringbuffer/frontend.h
index 9850d71..d0f6db0 100644
--- a/src/common/ringbuffer/frontend.h
+++ b/src/common/ringbuffer/frontend.h
@@ -266,25 +266,49 @@
 static inline
 unsigned long lib_ring_buffer_get_records_lost_full(
 				const struct lttng_ust_ring_buffer_config *config __attribute__((unused)),
-				const struct lttng_ust_ring_buffer_ctx *ctx)
+				struct lttng_ust_ring_buffer *buf,
+				const struct lttng_ust_ring_buffer_ctx *ctx __attribute__((unused)),
+				unsigned int subbuf_idx,
+				struct lttng_ust_shm_handle *handle)
 {
-	return ctx->priv->records_lost_full;
+	struct commit_counters_cold *cc_cold;
+
+	cc_cold = shmp_index(handle, buf->commit_cold, subbuf_idx);
+	if (!cc_cold)
+		return 0;
+	return cc_cold->records_lost_full;
 }
 
 static inline
 unsigned long lib_ring_buffer_get_records_lost_wrap(
 				const struct lttng_ust_ring_buffer_config *config __attribute__((unused)),
-				const struct lttng_ust_ring_buffer_ctx *ctx)
+				struct lttng_ust_ring_buffer *buf,
+				const struct lttng_ust_ring_buffer_ctx *ctx __attribute__((unused)),
+				unsigned int subbuf_idx,
+				struct lttng_ust_shm_handle *handle)
 {
-	return ctx->priv->records_lost_wrap;
+	struct commit_counters_cold *cc_cold;
+
+	cc_cold = shmp_index(handle, buf->commit_cold, subbuf_idx);
+	if (!cc_cold)
+		return 0;
+	return cc_cold->records_lost_wrap;
 }
 
 static inline
 unsigned long lib_ring_buffer_get_records_lost_big(
 				const struct lttng_ust_ring_buffer_config *config __attribute__((unused)),
-				const struct lttng_ust_ring_buffer_ctx *ctx)
+				struct lttng_ust_ring_buffer *buf,
+				const struct lttng_ust_ring_buffer_ctx *ctx __attribute__((unused)),
+				unsigned int subbuf_idx,
+				struct lttng_ust_shm_handle *handle)
 {
-	return ctx->priv->records_lost_big;
+	struct commit_counters_cold *cc_cold;
+
+	cc_cold = shmp_index(handle, buf->commit_cold, subbuf_idx);
+	if (!cc_cold)
+		return 0;
+	return cc_cold->records_lost_big;
 }
 
 static inline
diff --git a/src/common/ringbuffer/frontend_types.h b/src/common/ringbuffer/frontend_types.h
index 1b0e1a0..4fa9953 100644
--- a/src/common/ringbuffer/frontend_types.h
+++ b/src/common/ringbuffer/frontend_types.h
@@ -80,10 +80,20 @@
 	char padding[RB_COMMIT_COUNT_HOT_PADDING];
 } __attribute__((aligned(CAA_CACHE_LINE_SIZE)));
 
-/* Per-subbuffer commit counters used only on cold paths */
-#define RB_COMMIT_COUNT_COLD_PADDING	24
+/*
+ * Per-subbuffer commit counters used only on cold paths.
+ * No more padding left on 64-bit.
+ */
+#define RB_COMMIT_COUNT_COLD_PADDING	(24 - (3 * sizeof(unsigned long)))
 struct commit_counters_cold {
 	union v_atomic cc_sb;		/* Incremented _once_ at sb switch */
+	/*
+	 * Communicate records lost between space reservation at end of
+	 * sub-buffer and last commit of sub-buffer.
+	 */
+	unsigned long records_lost_full;
+	unsigned long records_lost_wrap;
+	unsigned long records_lost_big;
 	char padding[RB_COMMIT_COUNT_COLD_PADDING];
 } __attribute__((aligned(CAA_CACHE_LINE_SIZE)));
 
diff --git a/src/common/ringbuffer/ring_buffer_frontend.c b/src/common/ringbuffer/ring_buffer_frontend.c
index 5dcc0be..0d58d3b 100644
--- a/src/common/ringbuffer/ring_buffer_frontend.c
+++ b/src/common/ringbuffer/ring_buffer_frontend.c
@@ -1810,6 +1810,7 @@
 	const struct lttng_ust_ring_buffer_config *config = &chan->backend.config;
 	unsigned long oldidx = subbuf_index(offsets->old - 1, chan);
 	unsigned long commit_count, padding_size, data_size;
+	struct commit_counters_cold *cc_cold;
 	struct commit_counters_hot *cc_hot;
 	uint64_t *ts_end;
 
@@ -1821,15 +1822,21 @@
 	ts_end = shmp_index(handle, buf->ts_end, oldidx);
 	if (!ts_end)
 		return;
+	cc_cold = shmp_index(handle, buf->commit_cold, oldidx);
+	if (!cc_cold)
+		return;
 	/*
 	 * This is the last space reservation in that sub-buffer before
 	 * it gets delivered. This provides exclusive access to write to
-	 * this sub-buffer's ts_end. There are also no concurrent
-	 * readers of that ts_end because delivery of that sub-buffer is
-	 * postponed until the commit counter is incremented for the
-	 * current space reservation.
+	 * this sub-buffer's ts_end and records_lost fields. There are
+	 * also no concurrent readers of that ts_end because delivery of
+	 * that sub-buffer is postponed until the commit counter is
+	 * incremented for the current space reservation.
 	 */
 	*ts_end = ctx->priv->timestamp;
+	cc_cold->records_lost_full = ctx->priv->records_lost_full;
+	cc_cold->records_lost_wrap = ctx->priv->records_lost_wrap;
+	cc_cold->records_lost_big = ctx->priv->records_lost_big;
 
 	/*
 	 * Order all writes to buffer and store to ts_end before the commit
@@ -1903,6 +1910,7 @@
 				    struct lttng_ust_shm_handle *handle)
 {
 	const struct lttng_ust_ring_buffer_config *config = &chan->backend.config;
+	struct commit_counters_cold *cc_cold;
 	unsigned long endidx, data_size;
 	uint64_t *ts_end;
 
@@ -1913,15 +1921,21 @@
 	ts_end = shmp_index(handle, buf->ts_end, endidx);
 	if (!ts_end)
 		return;
+	cc_cold = shmp_index(handle, buf->commit_cold, endidx);
+	if (!cc_cold)
+		return;
 	/*
 	 * This is the last space reservation in that sub-buffer before
 	 * it gets delivered. This provides exclusive access to write to
-	 * this sub-buffer's ts_end. There are also no concurrent
-	 * readers of that ts_end because delivery of that sub-buffer is
-	 * postponed until the commit counter is incremented for the
-	 * current space reservation.
+	 * this sub-buffer's ts_end and records_lost fields. There are
+	 * also no concurrent readers of that ts_end because delivery of
+	 * that sub-buffer is postponed until the commit counter is
+	 * incremented for the current space reservation.
 	 */
 	*ts_end = ctx->priv->timestamp;
+	cc_cold->records_lost_full = ctx->priv->records_lost_full;
+	cc_cold->records_lost_wrap = ctx->priv->records_lost_wrap;
+	cc_cold->records_lost_big = ctx->priv->records_lost_big;
 }
 
 /*

